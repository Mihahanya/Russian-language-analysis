{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole text size: 71461745 so used 71461745\n",
      "Alphabet length: 2252\n",
      "Alphabet: \u0001\u0007\u0019 !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥¦§©ª«¬­®¯°±²³´µ·¹º»¼½¾¿×ßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿāăąćĉčđēĕėęěĝğģĥħĩīĭįıĵķĺļłńņňŋōŏőœŕřśŝşšţťũūŭůųźżžƒƙơưǀǁǂǃǎǐǒǔǚǝǧǫǯǰǹțȡȵȶɐɑɒɓɔɕɖɗɘəɛɜɞɟɠɡɢɣɤɥɦɧɨɪɫɬɭɮɯɰɱɲɳɴɵɶɸɹɺɻɽɾɿʀʁʂʃʄʈʉʊʋʌʍʎʏʐʑʒʔʕʘʙʛʜʝʟʡʢʣʤʥʦʧʰʱʲʷʹʻʼʿˀ˄˅ˆˇˈˉˊˋˌˍˎˏːˑ˒˓˔˘˜˟ˠˤ˥˦˧˨˩˸̶̝̞̟̠̣̥̩̪̬̮̯̹̻͇͍̀́̂̃̄̅̆̇̈̊̌̍̽͂̚͘͡΄άέήίαβγδεζηθικλμνξοπρςστυφχψωόύώϑϕϙϝϥабвгдежзийклмнопрстуфхцчшщъыьэюяёєѕіїјљњќўџѣѧѭѳѵѹҍҏґғҗҙқҡңҥҩҫүұҳҷҹһӂӄӈӏӑӗәӣӥӧөӱӳӷӽԓԥ՝աբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆևְֱֲֳִֵֶַָֹֻּֿׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשת׳،؟ءآأؤإئابةتثجحخدذرزسشصطظعغفقكلمنهوىئَُِْ٠٢٣پچڈڌژڤکگںہۋیےەܘވިँंइकगचटडतथदनपफबभमयरलवशसह़ािीुूेैॉो्ংআইউকডপফবমযল়ািুৰਚਜਫਰਵਸ਼ਿੀੌફળવિିୱகடதநபமயலழவாிீு்విಠವಿടവിවිกงฉดตทธนบผพภมยรฤวศษสาิีุเไ่้์ວິཝིကခဂဒနပမယဝာိီေး်ျြွაბგდევთიკლმნოპრსუფქცჭჱჽውᐅᐉᜏᜒលវិᠠᠢᠣᠨᠩᠮᠰᠴᠵᠸᡅᡝᡠᡤᡨᡯᡳᡵᤘᤡᥝᨓᨗᴙᵒᶟḍḥṅṇṛṟṣṭṳẓạảấậắẽếểệịọộớờởợụứửữựỹἀἁἃἄἐἔἡἱἴἷὁὄὐὑὰὴὶὸᾶ᾿ῆ῎ῖῦῶῷ​‌‍‎‏‐‑‒–—―‘’‚“”„†‡•…‬‰′″‿⁄⁠⁰⁴⁵⁶⁷⁸⁹ⁿ₂₣₥₪€₴₸₹₺₽℃№℘ℛ℠™ℳ⅓⅔⅛←↑→↓↗↘↵⇒⇧⇨∀∂∃∆∈∊∑−∙√∞∪∫≈≠≡≤≥⊂⋅⋯⌃⌊⌋⌑⌘⌛⌥⎯⎿␀␁␂␃␄␅␆␇␈␉␊␋␌␍␎␏␐␑␒␓␔␕␖␗␘␙␚␛␜␝␞␟␠␡␢␣①②③④⑤⑥ⓘ─│└├█▍▎▢▬▲▷►▼◄◌●☺♂♡⚓✅✓✔✗✝❒❤❯➜➤⟨⟩⟳⟶⠁⠅⠇⠊⠋⠍⠑⠕⠖⠗⠙⠚⠝⠠⠡⠢⠥⠫⠱⠺⬇⬊⬋⬝ⰲⱱⲟⲧⲩⴝⴰⴴⴻⵉⵎⵏⵖⵜⵥ、。《》あいうがきぎくけこごしじせそたちっつてとどなにはばふべほぼまもゃやょよりるんアィイウエオカキクケコサシスセソタダチツテデトドナニヌネノハヒフプヘペホマミムメモヤユョヨラリルレロワヰヱヲン・ーㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅈㅉㅊㅋㅌㅍㅎㅏㅐㅑㅓㅔㅕㅗㅘㅚㅛㅜㅝㅟㅠㅡㅢㅣ㑚一丁万三上不世丙东个中主义之乐乙九乞乡书买乳乸乾了予争事二五亥京亮人仁从仔他代令们件价份伊会伤似佇住体何余佛你佢佬佳使來例侗依侬係俄保個們倒倪偉偽傳傷儂儿先克兒兜入全八六兮兰共其典养内冇冊凉凡出分切列利到刷削剎劉力办加务劳務勞勿化北匮匱区區十午华協南卜卫印卷原厦去及友反发受口古只叫可台史号吃合名后吓吳吴吾呃员呢和咗咧哀哈哋响哭唐唔唱商問啥啲善喺嗨嘅嘢噶四囝回园図国图圀國圖土在地坐坡埋埔域基堂報場士壬壯外大天夫央头女她好如妙姐委娟媒媠子字学學孻安官定实客家寄寫导対将將小尔尚尽尾局展山嶺川州工己巳巻師常干平年广庆庚府度庸廖廣建开式张張当彙形彻很後得徛從微徹徽心必志快性总恁恩悟惹愛慢戊戌成我戒或战戚房手打扶投折护报拉招拨指挨捌捏推提搿撥播撮支改放政敕教敦整文斯新方旁族既日时明是時晉晋晚晡普暑暴曉書最會月有朋服期未本朴机李杯東极林枝某查校案桥條梳棘業極概構樂模橋機檢欽歇歌正歷殺毋母毒比毛民气気氣水汉汗汝沒沙沪河況法波活浊测济海涼港湘源漂漢潜澳濁灣火炒点热無照熱燾爭爾版特狀狮猪獅王珊珍班珮現球理瑚瑪生産用甩由甲申电画畀界略畫當畿疆癸發白百的皮盛盡目相盾省看県真着睛睡研础碗社福私种科称程稿究空立站竹笔第笺筆等答简箋箕管節範篡粤粵精糖系約紐級素紧細結給統經維網緊編繁约纽细经结给维罂罌网罗罢罷羅美群義翔老者而耳聊聞肉脂脱臺與舌舟船艦色节花苦英茶荔荣菇菩華萄萨萬著葡藏蘭號蚊蛇蜜蟆衞补表袓被裇補裡西要視親规言記詞詩話誌認語誰課論諗諡講謳識議讀讓认让议讯记讲讴论评识试诗话语读谁谂谥豬財買質贛财资赣起身車較輯车较辑辛辣辰边达过返这述迷這通造逻過道達邊邏那郎部都酉酒酻里重金針銀錐鏡锥镜門閂開閥閩關闡门闩问闽阀阮阿陆陈院除陰陳陽隻集難電靓靚面革音韻響頁頓頭頼題類页顿飛飞食飯飽餅養餘館饭饱饼馆首香馬騎马骑體高魔魚鸭麥麦麪黃點鼠龍龙龜龟ꃚꙋꚃ꞉ꞌꞎꦮꦶꮻ결경과구국극기나녀는다당도딸라로리마만말모문미방백베보부사상선세셻수시야어엄여오우위으의이장절정조죽지진차철칙키탁포한ﬁﯪﯮ️ﺍﺎﺏﺐﺑﺒﺕﺖﺗﺘﺙﺚﺛﺜﺝﺞﺟﺠﺡﺢﺣﺤﺥﺦﺧﺨﺩﺪﺫﺬﺭﺮﺯﺰﺱﺲﺳﺴﺵﺶﺷﺸﺹﺺﺻﺼﺽﺾﺿﻀﻁﻂﻃﻄﻅﻆﻇﻈﻍﻎﻏﻐﻑﻒﻓﻔﻕﻖﻗﻘﻙﻚﻛﻜﻝﻞﻟﻠﻡﻢﻣﻤﻥﻦﻧﻨﻩﻪﻫﻬﻭﻮﻱﻲﻳﻴ（），．：？￼�𐌀𐌁𐌂𐌃𐌄𐌅𐌆𐌇𐌈𐌉𐌊𐌋𐌌𐌍𐌎𐌏𐌐𐌑𐌒𐌓𐌔𐌕𐌖𐌗𐌘𐌙𐌚𐌜𐍅𐍈𝕏𝛾𝜹𝝎𝝓𝼄𝼅𝼆🄎🅭🅯🍋🍏🎁🎉🏁🏆🏻🏼👇👉👋👮💰💻🔄🔥🕹😁😉😊😏😞🙂🚩🤌🤔🤖🤗🤬🥁🥹𠊎𡥧𪜶\n"
     ]
    }
   ],
   "source": [
    "from data_base import *\n",
    "\n",
    "#data = DataBase(summary=True, max_size=3*10**6)\n",
    "#data = DataBase(summary=True, max_size=20000)\n",
    "data = DataBase(summary=True, max_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 399122\n",
      "['приветствую', 'всех', 'посетителей', 'и', 'пользователей', 'хабрахабра', 'сегодня', 'на', 'хабрахабре', 'открывается']\n",
      "[277163 250286 395530 326042 242869   1473  40262  82912 151772 276451]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "words_sequ = re.findall(r'[А-я]+', data.whole_text)\n",
    "words_to_ids = {v: i for i, v in enumerate(set(words_sequ))}\n",
    "ids_to_words = {v: i for i, v in words_to_ids.items()}\n",
    "words_sequ_ids = np.array([words_to_ids[word] for word in words_sequ])\n",
    "\n",
    "vocab_size = len(words_to_ids)\n",
    "\n",
    "print('unique words:', vocab_size)\n",
    "print(words_sequ[:10])\n",
    "print(words_sequ_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10002094/10002094 [01:10<00:00, 141984.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[277163. 250286. 395530. 326042. 242869.  40262.  82912. 151772. 276451.\n",
      "  225206.]\n",
      " [250286. 395530. 326042. 242869.   1473.  82912. 151772. 276451. 225206.\n",
      "  152532.]\n",
      " [395530. 326042. 242869.   1473.  40262. 151772. 276451. 225206. 152532.\n",
      "  330911.]\n",
      " [326042. 242869.   1473.  40262.  82912. 276451. 225206. 152532. 330911.\n",
      "  388377.]\n",
      " [242869.   1473.  40262.  82912. 151772. 225206. 152532. 330911. 388377.\n",
      "  103484.]\n",
      " [  1473.  40262.  82912. 151772. 276451. 152532. 330911. 388377. 103484.\n",
      "   91241.]\n",
      " [ 40262.  82912. 151772. 276451. 225206. 330911. 388377. 103484.  91241.\n",
      "   43016.]\n",
      " [ 82912. 151772. 276451. 225206. 152532. 388377. 103484.  91241.  43016.\n",
      "  161837.]\n",
      " [151772. 276451. 225206. 152532. 330911. 103484.  91241.  43016. 161837.\n",
      "  343586.]\n",
      " [276451. 225206. 152532. 330911. 388377.  91241.  43016. 161837. 343586.\n",
      "  168010.]]\n",
      "[  1473  40262  82912 151772 276451 225206 152532 330911 388377 103484]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "\n",
    "np.random.seed(80085)\n",
    "tf.random.set_seed(80085)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "db_size = len(words_sequ_ids) - window_size - window_size\n",
    "\n",
    "contexts = np.empty((db_size, window_size*2))\n",
    "labels = np.empty((db_size), dtype=np.uint32)\n",
    "\n",
    "for i in tqdm(range(window_size, len(words_sequ_ids)-window_size)):\n",
    "\tfr, to = i-window_size, i+window_size\n",
    "\n",
    "\tcontexts[i-window_size] = np.append(words_sequ_ids[fr:i], words_sequ_ids[i+1:to+1])\n",
    "\tlabels[i-window_size] = words_sequ_ids[i]\n",
    "\n",
    "#contexts = tf.constant(np.array(contexts))\n",
    "#labels = tf.sparse.from_dense(labels)\n",
    "\n",
    "print(contexts[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  102175232 \n",
      "                                                                 \n",
      " permute_7 (Permute)         multiple                  0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           multiple                  248       \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,700,024\n",
      "Trainable params: 102,700,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2Vec, self).__init__()\n",
    "\t\tself.target_embedding = tf.keras.layers.Embedding(vocab_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embedding_dim, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  input_length=window_size*2, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  name=\"w2v_embedding\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=1., seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.HeNormal())\n",
    "\t\tself.permute = tf.keras.layers.Permute((2, 1))\n",
    "\t\t#self.conv = tf.keras.layers.Conv1D(16, 4, 2, 'same')\n",
    "\t\tself.conv = tf.keras.layers.Conv1D(8, 3, 1, 'same')\n",
    "\t\tself.flatten = tf.keras.layers.Flatten()\n",
    "\t\tself.outp_emb = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "\t@tf.function\n",
    "\tdef call(self, context):\n",
    "\t\tx = self.target_embedding(context)\n",
    "\t\tx = self.permute(x)\n",
    "\t\tx = self.conv(x)\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = self.outp_emb(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "EMB_SIZE = 256\n",
    "model = Word2Vec(vocab_size, EMB_SIZE)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.007), \n",
    "\t\t\t  #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "\t\t\t  loss=tf.keras.losses.MeanSquaredError(), \n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "model.build((None, window_size*2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002094 10002094\n",
      "Epoch 1/5 (batch 1980/2000) [====================] loss: 0.24821178639915964, duration: 204.1s        \n",
      "Epoch 2/5 (batch 1980/2000) [====================] loss: 0.2231694937038157, duration: 201.39s        \n",
      "Epoch 3/5 (batch 1980/2000) [====================] loss: 0.19796254533234417, duration: 201.73s       \n",
      "Epoch 4/5 (batch 1980/2000) [====================] loss: 0.17505390033810383, duration: 201.95s       \n",
      "Epoch 5/5 (batch 1980/2000) [====================] loss: 0.14887305905742515, duration: 201.65s       \n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import random\n",
    "random.seed(80085)\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 5000\n",
    "use_data = len(contexts)\n",
    "#use_data = 6_000_000\n",
    "print(use_data, len(contexts))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} \", end='')\n",
    "\n",
    "    order = list(range(0, use_data-batch_size, batch_size))\n",
    "    loss_sum = 0\n",
    "    random.shuffle(order)\n",
    "    for i, bi in enumerate(order):\n",
    "        contexts_batch = contexts[bi:bi + batch_size]\n",
    "        labels_batch = labels[bi:bi + batch_size]\n",
    "\n",
    "        y_need = model.target_embedding(labels_batch)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(contexts_batch)\n",
    "            loss = model.loss(y_need, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        loss_sum += loss.numpy()\n",
    "\n",
    "        if i % (len(order)//100) == 0: \n",
    "            print(f\"\\rEpoch {epoch + 1}/{num_epochs} (batch {i}/{len(order)}) [\" + \n",
    "                  \"=\"*(int(i/len(order)*20 + 1)) + \n",
    "                  f\"] loss: {loss_sum/(i+1)}, duration: {round(time()-start, 2)}s       \", end='')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "улице \t проспекту \t 0.73765707 \t 0.015593559 \t 9.837156 \t 126.06534\n",
      "добрый \t хороший \t 10.817841 \t 0.6729502 \t 3.5435934 \t 46.59212\n",
      "поля \t луга \t 1.5671108 \t 0.2609946 \t 3.1992815 \t 40.95459\n",
      "стол \t бог \t -0.73816204 \t -0.048973426 \t 5.698789 \t 73.63722\n",
      "и \t у \t -0.39379156 \t -0.03423464 \t 8.860788 \t 118.17566\n",
      "долина \t мера \t 5.310046 \t 0.278921 \t 5.2418823 \t 67.50318\n",
      "хабрахабр \t хабр \t 4.5436306 \t 0.206181 \t 5.939909 \t 76.6445\n",
      "минус \t плюс \t 2.362055 \t 0.36998698 \t 3.1062844 \t 37.76257\n",
      "александра \t александр \t 0.6335316 \t 0.013880782 \t 9.520362 \t 121.88539\n",
      "муху \t мух \t 4.763549 \t 0.28409997 \t 5.0123115 \t 63.943333\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "\n",
    "def get_emb(word):\n",
    "\tword_ind = words_to_ids[word]\n",
    "\treturn embeddings[word_ind]\n",
    "\n",
    "def dist_dot(word1, word2, n=False):\n",
    "\temb1, emb2 = get_emb(word1), get_emb(word2)\n",
    "\tl1, l2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n",
    "\n",
    "\tif n: return np.dot(emb1, emb2) / l1 / l2\n",
    "\tif not n: return np.dot(emb1, emb2)\n",
    "\n",
    "def dist_geom(word1, word2):\n",
    "\temb1, emb2 = get_emb(word1), get_emb(word2)\n",
    "\treturn np.linalg.norm(emb1-emb2)\n",
    "\n",
    "def dist_diff(word1, word2):\n",
    "\temb1, emb2 = get_emb(word1), get_emb(word2)\n",
    "\treturn np.abs(emb1-emb2).sum()\n",
    "\n",
    "\n",
    "def test_words(w1, w2):\n",
    "\tprint(w1, '\\t', w2, '\\t', dist_dot(w1, w2), '\\t', dist_dot(w1, w2, n=True), '\\t', dist_geom(w1, w2), '\\t', dist_diff(w1, w2))\n",
    "\n",
    "test_words('улице', 'проспекту')\n",
    "test_words('добрый', 'хороший')\n",
    "test_words('поля', 'луга')\n",
    "test_words('стол', 'бог')\n",
    "test_words('и', 'у')\n",
    "test_words('долина', 'мера')\n",
    "test_words('хабрахабр', 'хабр')\n",
    "test_words('минус', 'плюс')\n",
    "test_words('александра', 'александр')\n",
    "test_words('муху', 'мух')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_words = {v: i for i, v in words_to_ids.items()}\n",
    "\n",
    "word_emb = {ids_to_words[i]: embeddings[i] for i in range(len(ids_to_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хабр 0.0\n",
      "смерть 4.2780566\n",
      "христос 4.373799\n",
      "там 4.375686\n",
      "солнце 4.3894067\n",
      "нынче 4.4131556\n",
      "именно 4.450928\n",
      "мама 4.4603915\n",
      "тебе 4.475134\n",
      "например 4.495083\n"
     ]
    }
   ],
   "source": [
    "def diff(a, b):\n",
    "\treturn np.linalg.norm(a-b)\n",
    "\t#return np.sum(np.abs(a-b))\n",
    "\t#return 1 - np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n",
    "\n",
    "def top_n_nearest(emb, n):\n",
    "\treturn sorted(word_emb.items(), key=lambda x: diff(x[1], emb), reverse=False)[:n]\n",
    "\n",
    "def print_top_n_nearest(emb, n):\n",
    "\ttops = top_n_nearest(emb, n)\n",
    "\n",
    "\tfor top in tops:\n",
    "\t\tprint(top[0], diff(top[1], emb))\n",
    "\n",
    "print_top_n_nearest(get_emb('хабр'), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
