{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole text size: 71461745 so used 71461745\n",
      "Alphabet length: 2252\n",
      "Alphabet: \u0001\u0007\u0019 !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~¡¢£¤¥¦§©ª«¬­®¯°±²³´µ·¹º»¼½¾¿×ßàáâãäåæçèéêëìíîïðñòóôõö÷øùúûüýþÿāăąćĉčđēĕėęěĝğģĥħĩīĭįıĵķĺļłńņňŋōŏőœŕřśŝşšţťũūŭůųźżžƒƙơưǀǁǂǃǎǐǒǔǚǝǧǫǯǰǹțȡȵȶɐɑɒɓɔɕɖɗɘəɛɜɞɟɠɡɢɣɤɥɦɧɨɪɫɬɭɮɯɰɱɲɳɴɵɶɸɹɺɻɽɾɿʀʁʂʃʄʈʉʊʋʌʍʎʏʐʑʒʔʕʘʙʛʜʝʟʡʢʣʤʥʦʧʰʱʲʷʹʻʼʿˀ˄˅ˆˇˈˉˊˋˌˍˎˏːˑ˒˓˔˘˜˟ˠˤ˥˦˧˨˩˸̶̝̞̟̠̣̥̩̪̬̮̯̹̻͇͍̀́̂̃̄̅̆̇̈̊̌̍̽͂̚͘͡΄άέήίαβγδεζηθικλμνξοπρςστυφχψωόύώϑϕϙϝϥабвгдежзийклмнопрстуфхцчшщъыьэюяёєѕіїјљњќўџѣѧѭѳѵѹҍҏґғҗҙқҡңҥҩҫүұҳҷҹһӂӄӈӏӑӗәӣӥӧөӱӳӷӽԓԥ՝աբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆևְֱֲֳִֵֶַָֹֻּֿׁׂאבגדהוזחטיךכלםמןנסעףפץצקרשת׳،؟ءآأؤإئابةتثجحخدذرزسشصطظعغفقكلمنهوىئَُِْ٠٢٣پچڈڌژڤکگںہۋیےەܘވިँंइकगचटडतथदनपफबभमयरलवशसह़ािीुूेैॉो्ংআইউকডপফবমযল়ািুৰਚਜਫਰਵਸ਼ਿੀੌફળવિିୱகடதநபமயலழவாிீு்విಠವಿടവിවිกงฉดตทธนบผพภมยรฤวศษสาิีุเไ่้์ວິཝིကခဂဒနပမယဝာိီေး်ျြွაბგდევთიკლმნოპრსუფქცჭჱჽውᐅᐉᜏᜒលវិᠠᠢᠣᠨᠩᠮᠰᠴᠵᠸᡅᡝᡠᡤᡨᡯᡳᡵᤘᤡᥝᨓᨗᴙᵒᶟḍḥṅṇṛṟṣṭṳẓạảấậắẽếểệịọộớờởợụứửữựỹἀἁἃἄἐἔἡἱἴἷὁὄὐὑὰὴὶὸᾶ᾿ῆ῎ῖῦῶῷ​‌‍‎‏‐‑‒–—―‘’‚“”„†‡•…‬‰′″‿⁄⁠⁰⁴⁵⁶⁷⁸⁹ⁿ₂₣₥₪€₴₸₹₺₽℃№℘ℛ℠™ℳ⅓⅔⅛←↑→↓↗↘↵⇒⇧⇨∀∂∃∆∈∊∑−∙√∞∪∫≈≠≡≤≥⊂⋅⋯⌃⌊⌋⌑⌘⌛⌥⎯⎿␀␁␂␃␄␅␆␇␈␉␊␋␌␍␎␏␐␑␒␓␔␕␖␗␘␙␚␛␜␝␞␟␠␡␢␣①②③④⑤⑥ⓘ─│└├█▍▎▢▬▲▷►▼◄◌●☺♂♡⚓✅✓✔✗✝❒❤❯➜➤⟨⟩⟳⟶⠁⠅⠇⠊⠋⠍⠑⠕⠖⠗⠙⠚⠝⠠⠡⠢⠥⠫⠱⠺⬇⬊⬋⬝ⰲⱱⲟⲧⲩⴝⴰⴴⴻⵉⵎⵏⵖⵜⵥ、。《》あいうがきぎくけこごしじせそたちっつてとどなにはばふべほぼまもゃやょよりるんアィイウエオカキクケコサシスセソタダチツテデトドナニヌネノハヒフプヘペホマミムメモヤユョヨラリルレロワヰヱヲン・ーㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅈㅉㅊㅋㅌㅍㅎㅏㅐㅑㅓㅔㅕㅗㅘㅚㅛㅜㅝㅟㅠㅡㅢㅣ㑚一丁万三上不世丙东个中主义之乐乙九乞乡书买乳乸乾了予争事二五亥京亮人仁从仔他代令们件价份伊会伤似佇住体何余佛你佢佬佳使來例侗依侬係俄保個們倒倪偉偽傳傷儂儿先克兒兜入全八六兮兰共其典养内冇冊凉凡出分切列利到刷削剎劉力办加务劳務勞勿化北匮匱区區十午华協南卜卫印卷原厦去及友反发受口古只叫可台史号吃合名后吓吳吴吾呃员呢和咗咧哀哈哋响哭唐唔唱商問啥啲善喺嗨嘅嘢噶四囝回园図国图圀國圖土在地坐坡埋埔域基堂報場士壬壯外大天夫央头女她好如妙姐委娟媒媠子字学學孻安官定实客家寄寫导対将將小尔尚尽尾局展山嶺川州工己巳巻師常干平年广庆庚府度庸廖廣建开式张張当彙形彻很後得徛從微徹徽心必志快性总恁恩悟惹愛慢戊戌成我戒或战戚房手打扶投折护报拉招拨指挨捌捏推提搿撥播撮支改放政敕教敦整文斯新方旁族既日时明是時晉晋晚晡普暑暴曉書最會月有朋服期未本朴机李杯東极林枝某查校案桥條梳棘業極概構樂模橋機檢欽歇歌正歷殺毋母毒比毛民气気氣水汉汗汝沒沙沪河況法波活浊测济海涼港湘源漂漢潜澳濁灣火炒点热無照熱燾爭爾版特狀狮猪獅王珊珍班珮現球理瑚瑪生産用甩由甲申电画畀界略畫當畿疆癸發白百的皮盛盡目相盾省看県真着睛睡研础碗社福私种科称程稿究空立站竹笔第笺筆等答简箋箕管節範篡粤粵精糖系約紐級素紧細結給統經維網緊編繁约纽细经结给维罂罌网罗罢罷羅美群義翔老者而耳聊聞肉脂脱臺與舌舟船艦色节花苦英茶荔荣菇菩華萄萨萬著葡藏蘭號蚊蛇蜜蟆衞补表袓被裇補裡西要視親规言記詞詩話誌認語誰課論諗諡講謳識議讀讓认让议讯记讲讴论评识试诗话语读谁谂谥豬財買質贛财资赣起身車較輯车较辑辛辣辰边达过返这述迷這通造逻過道達邊邏那郎部都酉酒酻里重金針銀錐鏡锥镜門閂開閥閩關闡门闩问闽阀阮阿陆陈院除陰陳陽隻集難電靓靚面革音韻響頁頓頭頼題類页顿飛飞食飯飽餅養餘館饭饱饼馆首香馬騎马骑體高魔魚鸭麥麦麪黃點鼠龍龙龜龟ꃚꙋꚃ꞉ꞌꞎꦮꦶꮻ결경과구국극기나녀는다당도딸라로리마만말모문미방백베보부사상선세셻수시야어엄여오우위으의이장절정조죽지진차철칙키탁포한ﬁﯪﯮ️ﺍﺎﺏﺐﺑﺒﺕﺖﺗﺘﺙﺚﺛﺜﺝﺞﺟﺠﺡﺢﺣﺤﺥﺦﺧﺨﺩﺪﺫﺬﺭﺮﺯﺰﺱﺲﺳﺴﺵﺶﺷﺸﺹﺺﺻﺼﺽﺾﺿﻀﻁﻂﻃﻄﻅﻆﻇﻈﻍﻎﻏﻐﻑﻒﻓﻔﻕﻖﻗﻘﻙﻚﻛﻜﻝﻞﻟﻠﻡﻢﻣﻤﻥﻦﻧﻨﻩﻪﻫﻬﻭﻮﻱﻲﻳﻴ（），．：？￼�𐌀𐌁𐌂𐌃𐌄𐌅𐌆𐌇𐌈𐌉𐌊𐌋𐌌𐌍𐌎𐌏𐌐𐌑𐌒𐌓𐌔𐌕𐌖𐌗𐌘𐌙𐌚𐌜𐍅𐍈𝕏𝛾𝜹𝝎𝝓𝼄𝼅𝼆🄎🅭🅯🍋🍏🎁🎉🏁🏆🏻🏼👇👉👋👮💰💻🔄🔥🕹😁😉😊😏😞🙂🚩🤌🤔🤖🤗🤬🥁🥹𠊎𡥧𪜶\n"
     ]
    }
   ],
   "source": [
    "from data_base import *\n",
    "\n",
    "#data = DataBase(summary=True, max_size=3*10**6)\n",
    "#data = DataBase(summary=True, max_size=20000)\n",
    "data = DataBase(summary=True, max_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 399122\n",
      "['приветствую', 'всех', 'посетителей', 'и', 'пользователей', 'хабрахабра', 'сегодня', 'на', 'хабрахабре', 'открывается']\n",
      "[ 39640 105075  34848 364670  91637 102528  49922 236742 180962 341033]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "words_sequ = re.findall(r'[А-я]+', data.whole_text)\n",
    "words_to_ids = {v: i for i, v in enumerate(set(words_sequ))}\n",
    "ids_to_words = {v: i for i, v in words_to_ids.items()}\n",
    "words_sequ_ids = np.array([words_to_ids[word] for word in words_sequ])\n",
    "\n",
    "vocab_size = len(words_to_ids)\n",
    "\n",
    "print('unique words:', vocab_size)\n",
    "print(words_sequ[:10])\n",
    "print(words_sequ_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10002094/10002094 [01:22<00:00, 121356.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39640. 105075.  34848. 364670.  91637.  49922. 236742. 180962. 341033.\n",
      "  145833.]\n",
      " [105075.  34848. 364670.  91637. 102528. 236742. 180962. 341033. 145833.\n",
      "  254139.]\n",
      " [ 34848. 364670.  91637. 102528.  49922. 180962. 341033. 145833. 254139.\n",
      "  323236.]\n",
      " [364670.  91637. 102528.  49922. 236742. 341033. 145833. 254139. 323236.\n",
      "  266483.]\n",
      " [ 91637. 102528.  49922. 236742. 180962. 145833. 254139. 323236. 266483.\n",
      "  132668.]\n",
      " [102528.  49922. 236742. 180962. 341033. 254139. 323236. 266483. 132668.\n",
      "  244496.]\n",
      " [ 49922. 236742. 180962. 341033. 145833. 323236. 266483. 132668. 244496.\n",
      "  278413.]\n",
      " [236742. 180962. 341033. 145833. 254139. 266483. 132668. 244496. 278413.\n",
      "  274178.]\n",
      " [180962. 341033. 145833. 254139. 323236. 132668. 244496. 278413. 274178.\n",
      "  167621.]\n",
      " [341033. 145833. 254139. 323236. 266483. 244496. 278413. 274178. 167621.\n",
      "  207202.]]\n",
      "[102528  49922 236742 180962 341033 145833 254139 323236 266483 132668]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "\n",
    "np.random.seed(80085)\n",
    "tf.random.set_seed(80085)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "db_size = len(words_sequ_ids) - window_size - window_size\n",
    "\n",
    "contexts = np.empty((db_size, window_size*2))\n",
    "labels = np.empty((db_size), dtype=np.uint32)\n",
    "\n",
    "for i in tqdm(range(window_size, len(words_sequ_ids)-window_size)):\n",
    "\tfr, to = i-window_size, i+window_size\n",
    "\n",
    "\tcontexts[i-window_size] = np.append(words_sequ_ids[fr:i], words_sequ_ids[i+1:to+1])\n",
    "\tlabels[i-window_size] = words_sequ_ids[i]\n",
    "\n",
    "#contexts = tf.constant(np.array(contexts))\n",
    "#labels = tf.sparse.from_dense(labels)\n",
    "\n",
    "print(contexts[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  25543808  \n",
      "                                                                 \n",
      " permute_6 (Permute)         multiple                  0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           multiple                  26        \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           multiple                  31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,543,865\n",
      "Trainable params: 25,543,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2Vec, self).__init__()\n",
    "\t\tself.target_embedding = tf.keras.layers.Embedding(vocab_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embedding_dim, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  input_length=window_size*2, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  name=\"w2v_embedding\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=1., seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.HeNormal())\n",
    "\t\tself.permute = tf.keras.layers.Permute((2, 1))\n",
    "\t\t#self.conv = tf.keras.layers.Conv1D(16, 4, 2, 'same')\n",
    "\t\tself.conv2d = tf.keras.layers.Conv2D(1, 5, 1, 'same')\n",
    "\t\tself.conv1d = tf.keras.layers.Conv1D(1, 3, 1, 'same')\n",
    "\t\t#self.flatten = tf.keras.layers.Flatten()\n",
    "\t\t#self.outp_emb = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "\t@tf.function\n",
    "\tdef call(self, context):\n",
    "\t\tx = self.target_embedding(context)\n",
    "\t\tx = self.permute(x)\n",
    "\t\tx = tf.expand_dims(x, -1)\n",
    "\t\tx = self.conv2d(x)\n",
    "\t\tx = tf.squeeze(x, -1)\n",
    "\t\tx = self.conv1d(x)\n",
    "\t\tx = tf.squeeze(x, -1)\n",
    "\t\t#x = self.flatten(x)\n",
    "\t\t#x = self.outp_emb(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "EMB_SIZE = 64\n",
    "model = Word2Vec(vocab_size, EMB_SIZE)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.007), \n",
    "\t\t\t  #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "\t\t\t  loss=tf.keras.losses.MeanSquaredError(), \n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "model.build((None, window_size*2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002094 10002094\n",
      "Epoch 1/20 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 (batch 1980/2000) [====================] loss: 0.269649513265149, duration: 134.2s          \n",
      "Epoch 2/20 (batch 1980/2000) [====================] loss: 0.17998655300243283, duration: 126.07s       \n",
      "Epoch 3/20 (batch 1980/2000) [====================] loss: 0.13154143288332906, duration: 130.97s       \n",
      "Epoch 4/20 (batch 1980/2000) [====================] loss: 0.09881907571784132, duration: 130.42s       \n",
      "Epoch 5/20 (batch 1980/2000) [====================] loss: 0.09169861502947303, duration: 131.88s       \n",
      "Epoch 6/20 (batch 1980/2000) [====================] loss: 0.08800231522994735, duration: 130.45s       \n",
      "Epoch 7/20 (batch 760/2000) [========] loss: 0.08399387153842133, duration: 52.25s       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CA81~1\\AppData\\Local\\Temp/ipykernel_15028/869291337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_SquaredDifferenceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;31m# The parens ensure that if grad is IndexedSlices, it'll get multiplied by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m     \u001b[1;31m# Tensor (not a number like 2.0) which causes it to convert to Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m     \u001b[0mx_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m   if (isinstance(grad, ops.Tensor) and\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mscalar_mul\u001b[1;34m(scalar, x, name)\u001b[0m\n\u001b[0;32m    625\u001b[0m           gen_math_ops.mul(scalar, x.values, name), x.indices, x.dense_shape)\n\u001b[0;32m    626\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     raise ValueError(\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6573\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6574\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6575\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6576\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6577\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import random\n",
    "random.seed(80085)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 5000\n",
    "use_data = len(contexts)\n",
    "#use_data = 6_000_000\n",
    "print(use_data, len(contexts))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} \", end='')\n",
    "\n",
    "    order = list(range(0, use_data-batch_size, batch_size))\n",
    "    loss_sum = 0\n",
    "    random.shuffle(order)\n",
    "    for i, bi in enumerate(order):\n",
    "        contexts_batch = contexts[bi:bi + batch_size]\n",
    "        labels_batch = labels[bi:bi + batch_size]\n",
    "\n",
    "        y_need = model.target_embedding(labels_batch)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(contexts_batch)\n",
    "            loss = model.loss(y_need, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        loss_sum += loss.numpy()\n",
    "\n",
    "        if i % (len(order)//100) == 0: \n",
    "            print(f\"\\rEpoch {epoch + 1}/{num_epochs} (batch {i}/{len(order)}) [\" + \n",
    "                  \"=\"*(int(i/len(order)*20 + 1)) + \n",
    "                  f\"] loss: {loss_sum/(i+1)}, duration: {round(time()-start, 2)}s       \", end='')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "улице \t проспекту \t 10.201253 \t 0.98021823 \t 0.72426844 \t 4.8403635\n",
      "добрый \t хороший \t 11.124401 \t 0.99772 \t 0.23136586 \t 1.5366074\n",
      "поля \t луга \t 11.570486 \t 0.98237604 \t 0.8990715 \t 6.2793064\n",
      "стол \t бог \t 9.991661 \t 0.99640316 \t 0.3890715 \t 2.40561\n",
      "и \t у \t 11.423006 \t 0.99916005 \t 0.14655137 \t 0.9689626\n",
      "долина \t мера \t 8.692085 \t 0.9364416 \t 1.0963017 \t 5.4050713\n",
      "хабрахабр \t хабр \t 11.406256 \t 0.9805872 \t 0.67493916 \t 3.1213298\n",
      "минус \t плюс \t 11.0450325 \t 0.9832976 \t 0.6995335 \t 4.2028046\n",
      "александра \t александр \t 9.724991 \t 0.9918079 \t 0.710856 \t 4.6152024\n",
      "муху \t мух \t 9.309185 \t 0.97954166 \t 0.63431233 \t 4.3750424\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "word_emb = {ids_to_words[i]: embeddings[i] for i in range(len(ids_to_words))}\n",
    "\n",
    "def dist_dot(word1, word2, n=False):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\tl1, l2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n",
    "\n",
    "\tif n: return np.dot(emb1, emb2) / l1 / l2\n",
    "\tif not n: return np.dot(emb1, emb2)\n",
    "\n",
    "def dist_geom(word1, word2):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\treturn np.linalg.norm(emb1-emb2)\n",
    "\n",
    "def dist_diff(word1, word2):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\treturn np.abs(emb1-emb2).sum()\n",
    "\n",
    "\n",
    "def test_words(w1, w2):\n",
    "\tprint(w1, '\\t', w2, '\\t', dist_dot(w1, w2), '\\t', dist_dot(w1, w2, n=True), '\\t', dist_geom(w1, w2), '\\t', dist_diff(w1, w2))\n",
    "\n",
    "test_words('улице', 'проспекту')\n",
    "test_words('добрый', 'хороший')\n",
    "test_words('поля', 'луга')\n",
    "test_words('стол', 'бог')\n",
    "test_words('и', 'у')\n",
    "test_words('долина', 'мера')\n",
    "test_words('хабрахабр', 'хабр')\n",
    "test_words('минус', 'плюс')\n",
    "test_words('александра', 'александр')\n",
    "test_words('муху', 'мух')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хабр 0.0\n",
      "возможно 0.22632118\n",
      "вполне 0.23413058\n",
      "наташа 0.24314485\n",
      "отказался 0.24428968\n",
      "глядя 0.24634175\n",
      "уж 0.24634175\n",
      "оставим 0.24750638\n",
      "вышло 0.24777776\n",
      "похудела 0.24807732\n"
     ]
    }
   ],
   "source": [
    "def diff(a, b):\n",
    "\treturn np.linalg.norm(a-b)\n",
    "\t#return np.sum(np.abs(a-b))\n",
    "\t#return 1 - np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n",
    "\n",
    "def top_n_nearest(emb, n):\n",
    "\treturn sorted(word_emb.items(), key=lambda x: diff(x[1], emb), reverse=False)[:n]\n",
    "\n",
    "def print_top_n_nearest(emb, n):\n",
    "\ttops = top_n_nearest(emb, n)\n",
    "\n",
    "\tfor top in tops:\n",
    "\t\tprint(top[0], diff(top[1], emb))\n",
    "\n",
    "print_top_n_nearest(word_emb['хабр'], 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
