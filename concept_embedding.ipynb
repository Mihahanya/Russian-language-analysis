{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole text size: 71461745 so used 71461745\n",
      "Alphabet length: 2252\n",
      "Alphabet: \u0001\u0007\u0019 !\"#$%&'()*+,-./0123456789:;<=>?@[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~ÂƒÂÂ—Â˜Â¡Â¢Â£Â¤Â¥Â¦Â§Â©ÂªÂ«Â¬Â­Â®Â¯Â°Â±Â²Â³Â´ÂµÂ·Â¹ÂºÂ»Â¼Â½Â¾Â¿Ã—ÃŸÃ Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã·Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¾Ã¿ÄÄƒÄ…Ä‡Ä‰ÄÄ‘Ä“Ä•Ä—Ä™Ä›ÄÄŸÄ£Ä¥Ä§Ä©Ä«Ä­Ä¯Ä±ÄµÄ·ÄºÄ¼Å‚Å„Å†ÅˆÅ‹ÅÅÅ‘Å“Å•Å™Å›ÅÅŸÅ¡Å£Å¥Å©Å«Å­Å¯Å³ÅºÅ¼Å¾Æ’Æ™Æ¡Æ°Ç€ÇÇ‚ÇƒÇÇÇ’Ç”ÇšÇÇ§Ç«Ç¯Ç°Ç¹È›È¡ÈµÈ¶ÉÉ‘É’É“É”É•É–É—É˜É™É›ÉœÉÉŸÉ É¡É¢É£É¤É¥É¦É§É¨ÉªÉ«É¬É­É®É¯É°É±É²É³É´ÉµÉ¶É¸É¹ÉºÉ»É½É¾É¿Ê€ÊÊ‚ÊƒÊ„ÊˆÊ‰ÊŠÊ‹ÊŒÊÊÊÊÊ‘Ê’Ê”Ê•Ê˜Ê™Ê›ÊœÊÊŸÊ¡Ê¢Ê£Ê¤Ê¥Ê¦Ê§Ê°Ê±Ê²Ê·Ê¹Ê»Ê¼Ê¿Ë€Ë„Ë…Ë†Ë‡ËˆË‰ËŠË‹ËŒËËËËË‘Ë’Ë“Ë”Ë˜ËœËŸË Ë¤Ë¥Ë¦Ë§Ë¨Ë©Ë¸Ì€ÌÌ‚ÌƒÌ„Ì…Ì†Ì‡ÌˆÌŠÌŒÌÌšÌÌÌŸÌ Ì£Ì¥Ì©ÌªÌ¬Ì®Ì¯Ì¶Ì¹Ì»Ì½Í‚Í‡ÍÍ˜Í¡Î„Î¬Î­Î®Î¯Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏ‚ÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰ÏŒÏÏÏ‘Ï•Ï™ÏÏ¥Ğ°Ğ±Ğ²Ğ³Ğ´ĞµĞ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑÑ‘Ñ”Ñ•Ñ–Ñ—Ñ˜Ñ™ÑšÑœÑÑŸÑ£Ñ§Ñ­Ñ³ÑµÑ¹ÒÒÒ‘Ò“Ò—Ò™Ò›Ò¡Ò£Ò¥Ò©Ò«Ò¯Ò±Ò³Ò·Ò¹Ò»Ó‚Ó„ÓˆÓÓ‘Ó—Ó™Ó£Ó¥Ó§Ó©Ó±Ó³Ó·Ó½Ô“Ô¥ÕÕ¡Õ¢Õ£Õ¤Õ¥Õ¦Õ§Õ¨Õ©ÕªÕ«Õ¬Õ­Õ®Õ¯Õ°Õ±Õ²Õ³Õ´ÕµÕ¶Õ·Õ¸Õ¹ÕºÕ»Õ¼Õ½Õ¾Õ¿Ö€ÖÖ‚ÖƒÖ„Ö…Ö†Ö‡Ö°Ö±Ö²Ö³Ö´ÖµÖ¶Ö·Ö¸Ö¹Ö»Ö¼Ö¿××‚××‘×’×“×”×•×–×—×˜×™×š×›×œ×××Ÿ× ×¡×¢×£×¤×¥×¦×§×¨×©×ª×³ØŒØŸØ¡Ø¢Ø£Ø¤Ø¥Ø¦Ø§Ø¨Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠÙÙÙÙ’Ù”Ù Ù¢Ù£Ù¾Ú†ÚˆÚŒÚ˜Ú¤Ú©Ú¯ÚºÛÛ‹ÛŒÛ’Û•Ü˜ŞˆŞ¨à¤à¤‚à¤‡à¤•à¤—à¤šà¤Ÿà¤¡à¤¤à¤¥à¤¦à¤¨à¤ªà¤«à¤¬à¤­à¤®à¤¯à¤°à¤²à¤µà¤¶à¤¸à¤¹à¤¼à¤¾à¤¿à¥€à¥à¥‚à¥‡à¥ˆà¥‰à¥‹à¥à¦‚à¦†à¦‡à¦‰à¦•à¦¡à¦ªà¦«à¦¬à¦®à¦¯à¦²à¦¼à¦¾à¦¿à§à§°à¨šà¨œà¨«à¨°à¨µà¨¸à¨¼à¨¿à©€à©Œàª«àª³àªµàª¿à¬¿à­±à®•à®Ÿà®¤à®¨à®ªà®®à®¯à®²à®´à®µà®¾à®¿à¯€à¯à¯à°µà°¿à² à²µà²¿à´Ÿà´µà´¿à·€à·’à¸à¸‡à¸‰à¸”à¸•à¸—à¸˜à¸™à¸šà¸œà¸à¸ à¸¡à¸¢à¸£à¸¤à¸§à¸¨à¸©à¸ªà¸²à¸´à¸µà¸¸à¹€à¹„à¹ˆà¹‰à¹Œàº§àº´à½à½²á€€á€á€‚á€’á€”á€•á€™á€šá€á€¬á€­á€®á€±á€¸á€ºá€»á€¼á€½áƒáƒ‘áƒ’áƒ“áƒ”áƒ•áƒ—áƒ˜áƒ™áƒšáƒ›áƒœáƒáƒáƒ áƒ¡áƒ£áƒ¤áƒ¥áƒªáƒ­áƒ±áƒ½á‹á…á‰áœáœ’á›áœá·á  á ¢á £á ¨á ©á ®á °á ´á µá ¸á¡…á¡á¡ á¡¤á¡¨á¡¯á¡³á¡µá¤˜á¤¡á¥á¨“á¨—á´™áµ’á¶Ÿá¸á¸¥á¹…á¹‡á¹›á¹Ÿá¹£á¹­á¹³áº“áº¡áº£áº¥áº­áº¯áº½áº¿á»ƒá»‡á»‹á»á»™á»›á»á»Ÿá»£á»¥á»©á»­á»¯á»±á»¹á¼€á¼á¼ƒá¼„á¼á¼”á¼¡á¼±á¼´á¼·á½á½„á½á½‘á½°á½´á½¶á½¸á¾¶á¾¿á¿†á¿á¿–á¿¦á¿¶á¿·â€‹â€Œâ€â€â€â€â€‘â€’â€“â€”â€•â€˜â€™â€šâ€œâ€â€â€ â€¡â€¢â€¦â€¬â€°â€²â€³â€¿â„â â°â´âµâ¶â·â¸â¹â¿â‚‚â‚£â‚¥â‚ªâ‚¬â‚´â‚¸â‚¹â‚ºâ‚½â„ƒâ„–â„˜â„›â„ â„¢â„³â…“â…”â…›â†â†‘â†’â†“â†—â†˜â†µâ‡’â‡§â‡¨âˆ€âˆ‚âˆƒâˆ†âˆˆâˆŠâˆ‘âˆ’âˆ™âˆšâˆâˆªâˆ«â‰ˆâ‰ â‰¡â‰¤â‰¥âŠ‚â‹…â‹¯âŒƒâŒŠâŒ‹âŒ‘âŒ˜âŒ›âŒ¥â¯â¿â€ââ‚âƒâ„â…â†â‡âˆâ‰âŠâ‹âŒâââââ‘â’â“â”â•â–â—â˜â™âšâ›âœâââŸâ â¡â¢â£â‘ â‘¡â‘¢â‘£â‘¤â‘¥â“˜â”€â”‚â””â”œâ–ˆâ–â–â–¢â–¬â–²â–·â–ºâ–¼â—„â—Œâ—â˜ºâ™‚â™¡âš“âœ…âœ“âœ”âœ—âœâ’â¤â¯âœâ¤âŸ¨âŸ©âŸ³âŸ¶â â …â ‡â Šâ ‹â â ‘â •â –â —â ™â šâ â  â ¡â ¢â ¥â «â ±â ºâ¬‡â¬Šâ¬‹â¬â°²â±±â²Ÿâ²§â²©â´â´°â´´â´»âµ‰âµâµâµ–âµœâµ¥ã€ã€‚ã€Šã€‹ã‚ã„ã†ãŒãããã‘ã“ã”ã—ã˜ã›ããŸã¡ã£ã¤ã¦ã¨ã©ãªã«ã¯ã°ãµã¹ã»ã¼ã¾ã‚‚ã‚ƒã‚„ã‚‡ã‚ˆã‚Šã‚‹ã‚“ã‚¢ã‚£ã‚¤ã‚¦ã‚¨ã‚ªã‚«ã‚­ã‚¯ã‚±ã‚³ã‚µã‚·ã‚¹ã‚»ã‚½ã‚¿ãƒ€ãƒãƒ„ãƒ†ãƒ‡ãƒˆãƒ‰ãƒŠãƒ‹ãƒŒãƒãƒãƒãƒ’ãƒ•ãƒ—ãƒ˜ãƒšãƒ›ãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ¤ãƒ¦ãƒ§ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ°ãƒ±ãƒ²ãƒ³ãƒ»ãƒ¼ã„±ã„²ã„´ã„·ã„¸ã„¹ã…ã…‚ã…ƒã……ã…†ã…ˆã…‰ã…Šã…‹ã…Œã…ã…ã…ã…ã…‘ã…“ã…”ã…•ã…—ã…˜ã…šã…›ã…œã…ã…Ÿã… ã…¡ã…¢ã…£ã‘šä¸€ä¸ä¸‡ä¸‰ä¸Šä¸ä¸–ä¸™ä¸œä¸ªä¸­ä¸»ä¹‰ä¹‹ä¹ä¹™ä¹ä¹ä¹¡ä¹¦ä¹°ä¹³ä¹¸ä¹¾äº†äºˆäº‰äº‹äºŒäº”äº¥äº¬äº®äººä»ä»ä»”ä»–ä»£ä»¤ä»¬ä»¶ä»·ä»½ä¼Šä¼šä¼¤ä¼¼ä½‡ä½ä½“ä½•ä½™ä½›ä½ ä½¢ä½¬ä½³ä½¿ä¾†ä¾‹ä¾—ä¾ä¾¬ä¿‚ä¿„ä¿å€‹å€‘å€’å€ªå‰å½å‚³å‚·å„‚å„¿å…ˆå…‹å…’å…œå…¥å…¨å…«å…­å…®å…°å…±å…¶å…¸å…»å†…å†‡å†Šå‡‰å‡¡å‡ºåˆ†åˆ‡åˆ—åˆ©åˆ°åˆ·å‰Šå‰åŠ‰åŠ›åŠåŠ åŠ¡åŠ³å‹™å‹å‹¿åŒ–åŒ—åŒ®åŒ±åŒºå€ååˆåå”å—åœå«å°å·åŸå¦å»åŠå‹åå‘å—å£å¤åªå«å¯å°å²å·åƒåˆååå“å³å´å¾å‘ƒå‘˜å‘¢å’Œå’—å’§å“€å“ˆå“‹å“å“­å”å””å”±å•†å•å•¥å•²å–„å–ºå—¨å˜…å˜¢å™¶å››å›å›å›­å›³å›½å›¾åœ€åœ‹åœ–åœŸåœ¨åœ°åå¡åŸ‹åŸ”åŸŸåŸºå ‚å ±å ´å£«å£¬å£¯å¤–å¤§å¤©å¤«å¤®å¤´å¥³å¥¹å¥½å¦‚å¦™å§å§”å¨Ÿåª’åª å­å­—å­¦å­¸å­»å®‰å®˜å®šå®å®¢å®¶å¯„å¯«å¯¼å¯¾å°†å°‡å°å°”å°šå°½å°¾å±€å±•å±±å¶ºå·å·å·¥å·±å·³å·»å¸«å¸¸å¹²å¹³å¹´å¹¿åº†åºšåºœåº¦åº¸å»–å»£å»ºå¼€å¼å¼ å¼µå½“å½™å½¢å½»å¾ˆå¾Œå¾—å¾›å¾å¾®å¾¹å¾½å¿ƒå¿…å¿—å¿«æ€§æ€»ææ©æ‚Ÿæƒ¹æ„›æ…¢æˆŠæˆŒæˆæˆ‘æˆ’æˆ–æˆ˜æˆšæˆ¿æ‰‹æ‰“æ‰¶æŠ•æŠ˜æŠ¤æŠ¥æ‹‰æ‹›æ‹¨æŒ‡æŒ¨æŒææ¨ææ¿æ’¥æ’­æ’®æ”¯æ”¹æ”¾æ”¿æ••æ•™æ•¦æ•´æ–‡æ–¯æ–°æ–¹æ—æ—æ—¢æ—¥æ—¶æ˜æ˜¯æ™‚æ™‰æ™‹æ™šæ™¡æ™®æš‘æš´æ›‰æ›¸æœ€æœƒæœˆæœ‰æœ‹æœæœŸæœªæœ¬æœ´æœºææ¯æ±ææ—ææŸæŸ¥æ ¡æ¡ˆæ¡¥æ¢æ¢³æ£˜æ¥­æ¥µæ¦‚æ§‹æ¨‚æ¨¡æ©‹æ©Ÿæª¢æ¬½æ­‡æ­Œæ­£æ­·æ®ºæ¯‹æ¯æ¯’æ¯”æ¯›æ°‘æ°”æ°—æ°£æ°´æ±‰æ±—æ±æ²’æ²™æ²ªæ²³æ³æ³•æ³¢æ´»æµŠæµ‹æµæµ·æ¶¼æ¸¯æ¹˜æºæ¼‚æ¼¢æ½œæ¾³æ¿ç£ç«ç‚’ç‚¹çƒ­ç„¡ç…§ç†±ç‡¾çˆ­çˆ¾ç‰ˆç‰¹ç‹€ç‹®çŒªç…ç‹çŠçç­ç®ç¾çƒç†ç‘šç‘ªç”Ÿç”£ç”¨ç”©ç”±ç”²ç”³ç”µç”»ç•€ç•Œç•¥ç•«ç•¶ç•¿ç–†ç™¸ç™¼ç™½ç™¾çš„çš®ç››ç›¡ç›®ç›¸ç›¾çœçœ‹çœŒçœŸç€ç›ç¡ç ”ç¡€ç¢—ç¤¾ç¦ç§ç§ç§‘ç§°ç¨‹ç¨¿ç©¶ç©ºç«‹ç«™ç«¹ç¬”ç¬¬ç¬ºç­†ç­‰ç­”ç®€ç®‹ç®•ç®¡ç¯€ç¯„ç¯¡ç²¤ç²µç²¾ç³–ç³»ç´„ç´ç´šç´ ç´§ç´°çµçµ¦çµ±ç¶“ç¶­ç¶²ç·Šç·¨ç¹çº¦çº½ç»†ç»ç»“ç»™ç»´ç½‚ç½Œç½‘ç½—ç½¢ç½·ç¾…ç¾ç¾¤ç¾©ç¿”è€è€…è€Œè€³èŠèè‚‰è„‚è„±è‡ºèˆ‡èˆŒèˆŸèˆ¹è‰¦è‰²èŠ‚èŠ±è‹¦è‹±èŒ¶è”è£è‡è©è¯è„è¨è¬è‘—è‘¡è—è˜­è™ŸèšŠè›‡èœœèŸ†è¡è¡¥è¡¨è¢“è¢«è£‡è£œè£¡è¥¿è¦è¦–è¦ªè§„è¨€è¨˜è©è©©è©±èªŒèªèªèª°èª²è«–è«—è«¡è¬›è¬³è­˜è­°è®€è®“è®¤è®©è®®è®¯è®°è®²è®´è®ºè¯„è¯†è¯•è¯—è¯è¯­è¯»è°è°‚è°¥è±¬è²¡è²·è³ªè´›è´¢èµ„èµ£èµ·èº«è»Šè¼ƒè¼¯è½¦è¾ƒè¾‘è¾›è¾£è¾°è¾¹è¾¾è¿‡è¿”è¿™è¿°è¿·é€™é€šé€ é€»éé“é”é‚Šé‚é‚£éƒéƒ¨éƒ½é…‰é…’é…»é‡Œé‡é‡‘é‡éŠ€éŒé¡é”¥é•œé–€é–‚é–‹é–¥é–©é—œé—¡é—¨é—©é—®é—½é˜€é˜®é˜¿é™†é™ˆé™¢é™¤é™°é™³é™½éš»é›†é›£é›»é“éšé¢é©éŸ³éŸ»éŸ¿é é “é ­é ¼é¡Œé¡é¡µé¡¿é£›é£é£Ÿé£¯é£½é¤…é¤Šé¤˜é¤¨é¥­é¥±é¥¼é¦†é¦–é¦™é¦¬é¨é©¬éª‘é«”é«˜é­”é­šé¸­éº¥éº¦éºªé»ƒé»é¼ é¾é¾™é¾œé¾Ÿêƒšê™‹êšƒê‰êŒêê¦®ê¦¶ê®»ê²°ê²½ê³¼êµ¬êµ­ê·¹ê¸°ë‚˜ë…€ëŠ”ë‹¤ë‹¹ë„ë”¸ë¼ë¡œë¦¬ë§ˆë§Œë§ëª¨ë¬¸ë¯¸ë°©ë°±ë² ë³´ë¶€ì‚¬ìƒì„ ì„¸ì…»ìˆ˜ì‹œì•¼ì–´ì—„ì—¬ì˜¤ìš°ìœ„ìœ¼ì˜ì´ì¥ì ˆì •ì¡°ì£½ì§€ì§„ì°¨ì² ì¹™í‚¤íƒí¬í•œï€­ï‚‰ï‚œïƒ ïƒ¨ï£¿ï¬ï¯ªï¯®ï¸ïºïºïºïºïº‘ïº’ïº•ïº–ïº—ïº˜ïº™ïºšïº›ïºœïºïºïºŸïº ïº¡ïº¢ïº£ïº¤ïº¥ïº¦ïº§ïº¨ïº©ïºªïº«ïº¬ïº­ïº®ïº¯ïº°ïº±ïº²ïº³ïº´ïºµïº¶ïº·ïº¸ïº¹ïººïº»ïº¼ïº½ïº¾ïº¿ï»€ï»ï»‚ï»ƒï»„ï»…ï»†ï»‡ï»ˆï»ï»ï»ï»ï»‘ï»’ï»“ï»”ï»•ï»–ï»—ï»˜ï»™ï»šï»›ï»œï»ï»ï»Ÿï» ï»¡ï»¢ï»£ï»¤ï»¥ï»¦ï»§ï»¨ï»©ï»ªï»«ï»¬ï»­ï»®ï»±ï»²ï»³ï»´ï¼ˆï¼‰ï¼Œï¼ï¼šï¼Ÿï¿¼ï¿½ğŒ€ğŒğŒ‚ğŒƒğŒ„ğŒ…ğŒ†ğŒ‡ğŒˆğŒ‰ğŒŠğŒ‹ğŒŒğŒğŒğŒğŒğŒ‘ğŒ’ğŒ“ğŒ”ğŒ•ğŒ–ğŒ—ğŒ˜ğŒ™ğŒšğŒœğ…ğˆğ•ğ›¾ğœ¹ğğ“ğ¼„ğ¼…ğ¼†ğŸ„ğŸ…­ğŸ…¯ğŸ‹ğŸğŸğŸ‰ğŸğŸ†ğŸ»ğŸ¼ğŸ‘‡ğŸ‘‰ğŸ‘‹ğŸ‘®ğŸ’°ğŸ’»ğŸ”„ğŸ”¥ğŸ•¹ğŸ˜ğŸ˜‰ğŸ˜ŠğŸ˜ğŸ˜ğŸ™‚ğŸš©ğŸ¤ŒğŸ¤”ğŸ¤–ğŸ¤—ğŸ¤¬ğŸ¥ğŸ¥¹ğ Šğ¡¥§ğªœ¶\n"
     ]
    }
   ],
   "source": [
    "from data_base import *\n",
    "\n",
    "#data = DataBase(summary=True, max_size=3*10**6)\n",
    "#data = DataBase(summary=True, max_size=20000)\n",
    "data = DataBase(summary=True, max_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 399122\n",
      "['Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑ', 'Ğ²ÑĞµÑ…', 'Ğ¿Ğ¾ÑĞµÑ‚Ğ¸Ñ‚ĞµĞ»ĞµĞ¹', 'Ğ¸', 'Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹', 'Ñ…Ğ°Ğ±Ñ€Ğ°Ñ…Ğ°Ğ±Ñ€Ğ°', 'ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ', 'Ğ½Ğ°', 'Ñ…Ğ°Ğ±Ñ€Ğ°Ñ…Ğ°Ğ±Ñ€Ğµ', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ']\n",
      "[ 39640 105075  34848 364670  91637 102528  49922 236742 180962 341033]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "words_sequ = re.findall(r'[Ğ-Ñ]+', data.whole_text)\n",
    "words_to_ids = {v: i for i, v in enumerate(set(words_sequ))}\n",
    "ids_to_words = {v: i for i, v in words_to_ids.items()}\n",
    "words_sequ_ids = np.array([words_to_ids[word] for word in words_sequ])\n",
    "\n",
    "vocab_size = len(words_to_ids)\n",
    "\n",
    "print('unique words:', vocab_size)\n",
    "print(words_sequ[:10])\n",
    "print(words_sequ_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10002094/10002094 [01:22<00:00, 121356.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39640. 105075.  34848. 364670.  91637.  49922. 236742. 180962. 341033.\n",
      "  145833.]\n",
      " [105075.  34848. 364670.  91637. 102528. 236742. 180962. 341033. 145833.\n",
      "  254139.]\n",
      " [ 34848. 364670.  91637. 102528.  49922. 180962. 341033. 145833. 254139.\n",
      "  323236.]\n",
      " [364670.  91637. 102528.  49922. 236742. 341033. 145833. 254139. 323236.\n",
      "  266483.]\n",
      " [ 91637. 102528.  49922. 236742. 180962. 145833. 254139. 323236. 266483.\n",
      "  132668.]\n",
      " [102528.  49922. 236742. 180962. 341033. 254139. 323236. 266483. 132668.\n",
      "  244496.]\n",
      " [ 49922. 236742. 180962. 341033. 145833. 323236. 266483. 132668. 244496.\n",
      "  278413.]\n",
      " [236742. 180962. 341033. 145833. 254139. 266483. 132668. 244496. 278413.\n",
      "  274178.]\n",
      " [180962. 341033. 145833. 254139. 323236. 132668. 244496. 278413. 274178.\n",
      "  167621.]\n",
      " [341033. 145833. 254139. 323236. 266483. 244496. 278413. 274178. 167621.\n",
      "  207202.]]\n",
      "[102528  49922 236742 180962 341033 145833 254139 323236 266483 132668]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from scipy.sparse.lil import lil_matrix\n",
    "\n",
    "np.random.seed(80085)\n",
    "tf.random.set_seed(80085)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "window_size = 5\n",
    "\n",
    "db_size = len(words_sequ_ids) - window_size - window_size\n",
    "\n",
    "contexts = np.empty((db_size, window_size*2))\n",
    "labels = np.empty((db_size), dtype=np.uint32)\n",
    "\n",
    "for i in tqdm(range(window_size, len(words_sequ_ids)-window_size)):\n",
    "\tfr, to = i-window_size, i+window_size\n",
    "\n",
    "\tcontexts[i-window_size] = np.append(words_sequ_ids[fr:i], words_sequ_ids[i+1:to+1])\n",
    "\tlabels[i-window_size] = words_sequ_ids[i]\n",
    "\n",
    "#contexts = tf.constant(np.array(contexts))\n",
    "#labels = tf.sparse.from_dense(labels)\n",
    "\n",
    "print(contexts[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"word2_vec_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " w2v_embedding (Embedding)   multiple                  25543808  \n",
      "                                                                 \n",
      " permute_6 (Permute)         multiple                  0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           multiple                  26        \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           multiple                  31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,543,865\n",
      "Trainable params: 25,543,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "\tdef __init__(self, vocab_size, embedding_dim):\n",
    "\t\tsuper(Word2Vec, self).__init__()\n",
    "\t\tself.target_embedding = tf.keras.layers.Embedding(vocab_size, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embedding_dim, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  input_length=window_size*2, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  name=\"w2v_embedding\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-1., maxval=1., seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=80085))\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #embeddings_initializer=tf.keras.initializers.HeNormal())\n",
    "\t\tself.permute = tf.keras.layers.Permute((2, 1))\n",
    "\t\t#self.conv = tf.keras.layers.Conv1D(16, 4, 2, 'same')\n",
    "\t\tself.conv2d = tf.keras.layers.Conv2D(1, 5, 1, 'same')\n",
    "\t\tself.conv1d = tf.keras.layers.Conv1D(1, 3, 1, 'same')\n",
    "\t\t#self.flatten = tf.keras.layers.Flatten()\n",
    "\t\t#self.outp_emb = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "\t@tf.function\n",
    "\tdef call(self, context):\n",
    "\t\tx = self.target_embedding(context)\n",
    "\t\tx = self.permute(x)\n",
    "\t\tx = tf.expand_dims(x, -1)\n",
    "\t\tx = self.conv2d(x)\n",
    "\t\tx = tf.squeeze(x, -1)\n",
    "\t\tx = self.conv1d(x)\n",
    "\t\tx = tf.squeeze(x, -1)\n",
    "\t\t#x = self.flatten(x)\n",
    "\t\t#x = self.outp_emb(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "EMB_SIZE = 64\n",
    "model = Word2Vec(vocab_size, EMB_SIZE)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.007), \n",
    "\t\t\t  #loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "\t\t\t  loss=tf.keras.losses.MeanSquaredError(), \n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "model.build((None, window_size*2))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002094 10002094\n",
      "Epoch 1/20 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 (batch 1980/2000) [====================] loss: 0.269649513265149, duration: 134.2s          \n",
      "Epoch 2/20 (batch 1980/2000) [====================] loss: 0.17998655300243283, duration: 126.07s       \n",
      "Epoch 3/20 (batch 1980/2000) [====================] loss: 0.13154143288332906, duration: 130.97s       \n",
      "Epoch 4/20 (batch 1980/2000) [====================] loss: 0.09881907571784132, duration: 130.42s       \n",
      "Epoch 5/20 (batch 1980/2000) [====================] loss: 0.09169861502947303, duration: 131.88s       \n",
      "Epoch 6/20 (batch 1980/2000) [====================] loss: 0.08800231522994735, duration: 130.45s       \n",
      "Epoch 7/20 (batch 760/2000) [========] loss: 0.08399387153842133, duration: 52.25s       "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CA81~1\\AppData\\Local\\Temp/ipykernel_15028/869291337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_need\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1111\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    158\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_SquaredDifferenceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[1;31m# The parens ensure that if grad is IndexedSlices, it'll get multiplied by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m     \u001b[1;31m# Tensor (not a number like 2.0) which causes it to convert to Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1614\u001b[1;33m     \u001b[0mx_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m   if (isinstance(grad, ops.Tensor) and\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mscalar_mul\u001b[1;34m(scalar, x, name)\u001b[0m\n\u001b[0;32m    625\u001b[0m           gen_math_ops.mul(scalar, x.values, name), x.indices, x.dense_shape)\n\u001b[0;32m    626\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m     raise ValueError(\n",
      "\u001b[1;32md:\\PROGRAMS\\python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6573\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6574\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6575\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   6576\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6577\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import random\n",
    "random.seed(80085)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 5000\n",
    "use_data = len(contexts)\n",
    "#use_data = 6_000_000\n",
    "print(use_data, len(contexts))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} \", end='')\n",
    "\n",
    "    order = list(range(0, use_data-batch_size, batch_size))\n",
    "    loss_sum = 0\n",
    "    random.shuffle(order)\n",
    "    for i, bi in enumerate(order):\n",
    "        contexts_batch = contexts[bi:bi + batch_size]\n",
    "        labels_batch = labels[bi:bi + batch_size]\n",
    "\n",
    "        y_need = model.target_embedding(labels_batch)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(contexts_batch)\n",
    "            loss = model.loss(y_need, y_pred)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        loss_sum += loss.numpy()\n",
    "\n",
    "        if i % (len(order)//100) == 0: \n",
    "            print(f\"\\rEpoch {epoch + 1}/{num_epochs} (batch {i}/{len(order)}) [\" + \n",
    "                  \"=\"*(int(i/len(order)*20 + 1)) + \n",
    "                  f\"] loss: {loss_sum/(i+1)}, duration: {round(time()-start, 2)}s       \", end='')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÑƒĞ»Ğ¸Ñ†Ğµ \t Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ñƒ \t 10.201253 \t 0.98021823 \t 0.72426844 \t 4.8403635\n",
      "Ğ´Ğ¾Ğ±Ñ€Ñ‹Ğ¹ \t Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ \t 11.124401 \t 0.99772 \t 0.23136586 \t 1.5366074\n",
      "Ğ¿Ğ¾Ğ»Ñ \t Ğ»ÑƒĞ³Ğ° \t 11.570486 \t 0.98237604 \t 0.8990715 \t 6.2793064\n",
      "ÑÑ‚Ğ¾Ğ» \t Ğ±Ğ¾Ğ³ \t 9.991661 \t 0.99640316 \t 0.3890715 \t 2.40561\n",
      "Ğ¸ \t Ñƒ \t 11.423006 \t 0.99916005 \t 0.14655137 \t 0.9689626\n",
      "Ğ´Ğ¾Ğ»Ğ¸Ğ½Ğ° \t Ğ¼ĞµÑ€Ğ° \t 8.692085 \t 0.9364416 \t 1.0963017 \t 5.4050713\n",
      "Ñ…Ğ°Ğ±Ñ€Ğ°Ñ…Ğ°Ğ±Ñ€ \t Ñ…Ğ°Ğ±Ñ€ \t 11.406256 \t 0.9805872 \t 0.67493916 \t 3.1213298\n",
      "Ğ¼Ğ¸Ğ½ÑƒÑ \t Ğ¿Ğ»ÑÑ \t 11.0450325 \t 0.9832976 \t 0.6995335 \t 4.2028046\n",
      "Ğ°Ğ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€Ğ° \t Ğ°Ğ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€ \t 9.724991 \t 0.9918079 \t 0.710856 \t 4.6152024\n",
      "Ğ¼ÑƒÑ…Ñƒ \t Ğ¼ÑƒÑ… \t 9.309185 \t 0.97954166 \t 0.63431233 \t 4.3750424\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "word_emb = {ids_to_words[i]: embeddings[i] for i in range(len(ids_to_words))}\n",
    "\n",
    "def dist_dot(word1, word2, n=False):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\tl1, l2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n",
    "\n",
    "\tif n: return np.dot(emb1, emb2) / l1 / l2\n",
    "\tif not n: return np.dot(emb1, emb2)\n",
    "\n",
    "def dist_geom(word1, word2):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\treturn np.linalg.norm(emb1-emb2)\n",
    "\n",
    "def dist_diff(word1, word2):\n",
    "\temb1, emb2 = word_emb[word1], word_emb[word2]\n",
    "\treturn np.abs(emb1-emb2).sum()\n",
    "\n",
    "\n",
    "def test_words(w1, w2):\n",
    "\tprint(w1, '\\t', w2, '\\t', dist_dot(w1, w2), '\\t', dist_dot(w1, w2, n=True), '\\t', dist_geom(w1, w2), '\\t', dist_diff(w1, w2))\n",
    "\n",
    "test_words('ÑƒĞ»Ğ¸Ñ†Ğµ', 'Ğ¿Ñ€Ğ¾ÑĞ¿ĞµĞºÑ‚Ñƒ')\n",
    "test_words('Ğ´Ğ¾Ğ±Ñ€Ñ‹Ğ¹', 'Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹')\n",
    "test_words('Ğ¿Ğ¾Ğ»Ñ', 'Ğ»ÑƒĞ³Ğ°')\n",
    "test_words('ÑÑ‚Ğ¾Ğ»', 'Ğ±Ğ¾Ğ³')\n",
    "test_words('Ğ¸', 'Ñƒ')\n",
    "test_words('Ğ´Ğ¾Ğ»Ğ¸Ğ½Ğ°', 'Ğ¼ĞµÑ€Ğ°')\n",
    "test_words('Ñ…Ğ°Ğ±Ñ€Ğ°Ñ…Ğ°Ğ±Ñ€', 'Ñ…Ğ°Ğ±Ñ€')\n",
    "test_words('Ğ¼Ğ¸Ğ½ÑƒÑ', 'Ğ¿Ğ»ÑÑ')\n",
    "test_words('Ğ°Ğ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€Ğ°', 'Ğ°Ğ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€')\n",
    "test_words('Ğ¼ÑƒÑ…Ñƒ', 'Ğ¼ÑƒÑ…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ñ…Ğ°Ğ±Ñ€ 0.0\n",
      "Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ 0.22632118\n",
      "Ğ²Ğ¿Ğ¾Ğ»Ğ½Ğµ 0.23413058\n",
      "Ğ½Ğ°Ñ‚Ğ°ÑˆĞ° 0.24314485\n",
      "Ğ¾Ñ‚ĞºĞ°Ğ·Ğ°Ğ»ÑÑ 0.24428968\n",
      "Ğ³Ğ»ÑĞ´Ñ 0.24634175\n",
      "ÑƒĞ¶ 0.24634175\n",
      "Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼ 0.24750638\n",
      "Ğ²Ñ‹ÑˆĞ»Ğ¾ 0.24777776\n",
      "Ğ¿Ğ¾Ñ…ÑƒĞ´ĞµĞ»Ğ° 0.24807732\n"
     ]
    }
   ],
   "source": [
    "def diff(a, b):\n",
    "\treturn np.linalg.norm(a-b)\n",
    "\t#return np.sum(np.abs(a-b))\n",
    "\t#return 1 - np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)\n",
    "\n",
    "def top_n_nearest(emb, n):\n",
    "\treturn sorted(word_emb.items(), key=lambda x: diff(x[1], emb), reverse=False)[:n]\n",
    "\n",
    "def print_top_n_nearest(emb, n):\n",
    "\ttops = top_n_nearest(emb, n)\n",
    "\n",
    "\tfor top in tops:\n",
    "\t\tprint(top[0], diff(top[1], emb))\n",
    "\n",
    "print_top_n_nearest(word_emb['Ñ…Ğ°Ğ±Ñ€'], 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
